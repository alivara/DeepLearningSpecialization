# Deep Learning
Hi anyone, in this repository i will upload all the project which i have done on deep learning, and hope to help you by the idea of coding.
Deep learning specialization projects

* Building your Deep Neural Network
  * Use non-linear units like ReLU to improve your model
  * Build a deeper neural network (with more than 1 hidden layer)
  * Implement an easy-to-use neural network class
* Deep Neural Network for Image Classification
  * Build and train a deep L-layer neural network, and apply it to supervised learning
* Logistic Regression with a Neural Network mindset
  * Build the general architecture of a learning algorithm, including:
    * Initializing parameters
    * Calculating the cost function and its gradient
    * Using an optimization algorithm (gradient descent) 
    * Gather all three functions above into a main model function, in the right order.
* Planar data classification with one hidden layer
  * Implement a 2-class classification neural network with a single hidden layer
  * Use units with a non-linear activation function, such as tanh
  * Compute the cross-entropy loss
  * Implement forward and backward propagation
  * Gradient Checking
  * Implement gradient checking to verify the accuracy of your backprop implementation.
* Initialization
  * Speed up the convergence of gradient descent
  * Increase the odds of gradient descent converging to a lower training (and generalization) error  
* Optimization Methods
  * Apply optimization methods such as (Stochastic) Gradient Descent, Momentum, RMSProp and Adam
  * Use random minibatches to accelerate convergence and improve optimization
  * Regularization
* Neural Style Transfer
  *	Implement the neural style transfer algorithm 
  *	Generate novel artistic images using your algorithm 
  *	Define the style cost function for Neural Style Transfer
  *	Define the content cost function for Neural Style Transfer
* Face Recognition
  *	Differentiate between face recognition and face verification
  *	Implement one-shot learning to solve a face recognition problem
  *	Apply the triplet loss function to learn a network's parameters in the context of face recognition
  *	Explain how to pose face recognition as a binary classification problem
  *	Map face images into 128-dimensional encodings using a pretrained model
  *	Perform face verification and face recognition with these encodings
* Building Recurrent Neural Network
  *	Define notation for building sequence models
  *	Describe the architecture of a basic RNN
  *	Identify the main components of an LSTM
  *	Implement backpropagation through time for a basic RNN and an LSTM
  *	Give examples of several types of RNN
*	Transformer Network
     *	Create positional encodings to capture sequential relationships in data
     *	Calculate scaled dot-product self-attention with word embeddings
     *	Implement masked multi-head attention
     *	Build and train a Transformer model
*	Character level language model - Dinosaurus Island
     *	Store text data for processing using an RNN 
     *	Build a character-level text generation model using an RNN
     *	Sample novel sequences in an RNN
     *	Explain the vanishing/exploding gradient problem in RNNs
     *	Apply gradient clipping as a solution for exploding gradients
* Transformer Pre-processing
     * Create visualizations to gain intuition on positional encodings
     * Visualize how positional encodings affect word embeddings
*	Emojify
     *	Create an embedding layer in Keras with pre-trained word vectors
     *	Explain the advantages and disadvantages of the GloVe algorithm
     *	Describe how negative sampling learns word vectors more efficiently than other methods
     *	Build a sentiment classifier using word embeddings
     *	Build and train a more sophisticated classifier using an LSTM
*	Improvise a Jazz Solo with an LSTM Network
     *	Apply an LSTM to a music generation task
     *	Generate your own jazz music with deep learning
     *	Use the flexible Functional API to create complex models
     *	Neural Machine Translation
     *	building a Neural Machine Translation (NMT) model to translate human-readable dates ("25th of June, 2009") into machine-readable dates ("2009-06-25"). 
     *	using an attention model, one of the most sophisticated sequence-to-sequence models.
*	Operations on Word Vectors
     *	Explain how word embeddings capture relationships between words
     *	Load pre-trained word vectors
     *	Measure similarity between word vectors using cosine similarity
     *	Use word embeddings to solve word analogy problems such as Man is to Woman as King is to __.
*	Transformer Network Application: Question Answering
     *	Perform extractive Question Answering 
     *	Fine-tune a pre-trained transformer model to a custom dataset
     *	Implement a QA model in TensorFlow and PyTorch
*	Transformer Network Application: Named-Entity Recognition
     *	Use tokenizers and pre-trained models from the HuggingFace Library.
     *	Fine-tune a pre-trained transformer model for Named-Entity Recognition
*	Trigger Word Detection
     *	Structure a speech recognition project
     *	Synthesize and process audio recordings to create train/dev datasets
     *	Train a trigger word detection model and make predictions
